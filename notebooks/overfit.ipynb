{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import json\n",
    "from vision_transformer import ViT, Trainer\n",
    "from torchvision.datasets import CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(root=\"../data\", download=True, transform=v2.ToTensor())\n",
    "dataset = Subset(dataset, range(1))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a10a7e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_size': 32,\n",
       " 'channels': 3,\n",
       " 'patch_size': 4,\n",
       " 'dim': 32,\n",
       " 'layers': 4,\n",
       " 'heads': 2,\n",
       " 'mlp_dim': 64,\n",
       " 'num_classes': 10}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_CONFIG = \"../configs/default.json\"\n",
    "\n",
    "with open(MODEL_CONFIG, \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (transformer): Transformer(\n",
       "    (encoders): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoder(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (to_qkv): Linear(in_features=32, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT(**model_config)\n",
    "vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=vit,\n",
    "    train_loader=dataloader,\n",
    "    epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 133.07it/s]2.29]\n",
      "100%|██████████| 1/1 [00:00<00:00, 158.78it/s]s, loss=2.13]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.15it/s]s, loss=1.97]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.40it/s]s, loss=1.82]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.28it/s]s, loss=1.69]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.45it/s]s, loss=1.58]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.05it/s]s, loss=1.48]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.74it/s]s, loss=1.39]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.90it/s]s, loss=1.32]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.49it/s]s, loss=1.26]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.54it/s]/s, loss=1.21]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 179.47it/s]/s, loss=1.17]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.38it/s]/s, loss=1.13]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.10it/s]/s, loss=1.09]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.49it/s]/s, loss=1.06]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 141.54it/s]/s, loss=1.03]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.17it/s]/s, loss=1]          \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.81it/s]/s, loss=0.975]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 152.12it/s]/s, loss=0.951]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 155.21it/s]/s, loss=0.929]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.74it/s]/s, loss=0.909]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.24it/s]/s, loss=0.89]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.40it/s]/s, loss=0.871]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.51it/s]/s, loss=0.854]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.41it/s]/s, loss=0.838]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.09it/s]/s, loss=0.822]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.15it/s]/s, loss=0.807]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.39it/s]/s, loss=0.793]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 159.26it/s]/s, loss=0.779]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.60it/s]/s, loss=0.766]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.81it/s]/s, loss=0.753]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.43it/s]/s, loss=0.74]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.42it/s]/s, loss=0.728]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.01it/s]/s, loss=0.716]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.72it/s]/s, loss=0.704]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.03it/s]/s, loss=0.693]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.60it/s]/s, loss=0.681]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.66it/s]/s, loss=0.67]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 157.70it/s]/s, loss=0.659]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.82it/s]/s, loss=0.648]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.22it/s]/s, loss=0.637]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.20it/s]/s, loss=0.626]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 157.19it/s]/s, loss=0.616]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 159.44it/s]/s, loss=0.605]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.87it/s]/s, loss=0.595]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.53it/s]/s, loss=0.585]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.03it/s]/s, loss=0.575]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 159.80it/s]/s, loss=0.565]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.29it/s]/s, loss=0.555]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.45it/s]/s, loss=0.546]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.46it/s]/s, loss=0.536]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.21it/s]/s, loss=0.527]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 159.08it/s]/s, loss=0.518]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 179.57it/s]/s, loss=0.509]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.19it/s]/s, loss=0.5]        \n",
      "100%|██████████| 1/1 [00:00<00:00, 121.63it/s]/s, loss=0.492]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.23it/s]/s, loss=0.484]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.79it/s]/s, loss=0.475]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 157.98it/s]/s, loss=0.467]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.71it/s]/s, loss=0.459]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.14it/s]/s, loss=0.451]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 159.57it/s]/s, loss=0.444]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.80it/s]/s, loss=0.436]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.64it/s]/s, loss=0.429]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.92it/s]/s, loss=0.422]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 158.95it/s]/s, loss=0.415]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.75it/s]/s, loss=0.408]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 157.83it/s]/s, loss=0.401]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 152.30it/s]/s, loss=0.394]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.51it/s]/s, loss=0.388]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.38it/s]/s, loss=0.382]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.90it/s]/s, loss=0.375]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 160.23it/s]/s, loss=0.369]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.07it/s]/s, loss=0.363]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.47it/s]/s, loss=0.357]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 144.53it/s]/s, loss=0.352]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.61it/s]/s, loss=0.346]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.58it/s]/s, loss=0.34]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.46it/s]/s, loss=0.335]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 150.83it/s]/s, loss=0.33]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.01it/s]/s, loss=0.324]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.58it/s]/s, loss=0.319]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 160.85it/s]/s, loss=0.314]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 158.05it/s]/s, loss=0.309]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.99it/s]/s, loss=0.305]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.10it/s]/s, loss=0.3]        \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.63it/s]/s, loss=0.295]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.12it/s]/s, loss=0.291]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.29it/s]/s, loss=0.286]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.67it/s]/s, loss=0.282]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.10it/s]/s, loss=0.278]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.24it/s]/s, loss=0.274]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.44it/s]/s, loss=0.269]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 140.87it/s]/s, loss=0.266]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 144.80it/s]/s, loss=0.262]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 130.36it/s]/s, loss=0.258]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 143.43it/s]/s, loss=0.254]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 144.87it/s]/s, loss=0.25]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 160.51it/s]/s, loss=0.247]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 146.72it/s]/s, loss=0.243]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.44it/s]t/s, loss=0.24]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 151.91it/s]t/s, loss=0.236]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.59it/s]t/s, loss=0.233]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.56it/s]t/s, loss=0.23]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.87it/s]t/s, loss=0.227]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.49it/s]t/s, loss=0.223]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 157.53it/s]t/s, loss=0.22]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 154.08it/s]t/s, loss=0.217]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 93.71it/s]it/s, loss=0.214]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 146.77it/s]t/s, loss=0.212]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.96it/s]t/s, loss=0.209]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.86it/s]t/s, loss=0.206]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.88it/s]t/s, loss=0.203]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 154.37it/s]t/s, loss=0.201]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.65it/s]t/s, loss=0.198]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 125.74it/s]t/s, loss=0.195]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 155.98it/s]t/s, loss=0.193]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.84it/s]t/s, loss=0.19]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 151.63it/s]t/s, loss=0.188]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.69it/s]t/s, loss=0.186]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.02it/s]t/s, loss=0.183]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.23it/s]t/s, loss=0.181]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.18it/s]t/s, loss=0.179]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.31it/s]t/s, loss=0.177]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 159.52it/s]t/s, loss=0.174]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.03it/s]t/s, loss=0.172]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.18it/s]t/s, loss=0.17]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.97it/s]t/s, loss=0.168]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.58it/s]t/s, loss=0.166]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.41it/s]t/s, loss=0.164]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.37it/s]t/s, loss=0.162]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.83it/s]t/s, loss=0.16]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.80it/s]t/s, loss=0.158]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.40it/s]t/s, loss=0.157]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.00it/s]t/s, loss=0.155]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.21it/s]t/s, loss=0.153]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 185.50it/s]t/s, loss=0.151]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.57it/s]t/s, loss=0.149]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.70it/s]t/s, loss=0.148]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.33it/s]t/s, loss=0.146]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.40it/s]t/s, loss=0.145]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.28it/s]t/s, loss=0.143]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.28it/s]t/s, loss=0.141]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 126.13it/s]t/s, loss=0.14]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.51it/s]t/s, loss=0.138]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.35it/s]t/s, loss=0.137]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.75it/s]t/s, loss=0.135]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.99it/s]t/s, loss=0.134]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.38it/s]t/s, loss=0.132]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 150.12it/s]t/s, loss=0.131]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.38it/s]t/s, loss=0.13]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.31it/s]t/s, loss=0.128]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.41it/s]t/s, loss=0.127]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.18it/s]t/s, loss=0.126]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.75it/s]t/s, loss=0.124]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.85it/s]t/s, loss=0.123]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.35it/s]t/s, loss=0.122]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.63it/s]t/s, loss=0.121]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 154.83it/s]t/s, loss=0.119]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.97it/s]t/s, loss=0.118]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 187.62it/s]t/s, loss=0.117]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.26it/s]t/s, loss=0.116]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 184.47it/s]t/s, loss=0.115]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 152.16it/s]t/s, loss=0.114]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.91it/s]t/s, loss=0.112]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.59it/s]t/s, loss=0.111]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.85it/s]t/s, loss=0.11]       \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.26it/s]t/s, loss=0.109]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 152.41it/s]t/s, loss=0.108]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.80it/s]t/s, loss=0.107]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.26it/s]t/s, loss=0.106]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 154.20it/s]t/s, loss=0.105]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.49it/s]t/s, loss=0.104]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.29it/s]t/s, loss=0.103]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.85it/s]t/s, loss=0.102]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 166.30it/s]t/s, loss=0.101]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.94it/s]t/s, loss=0.1]        \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.84it/s]t/s, loss=0.0995]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.77it/s]t/s, loss=0.0986]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.37it/s]t/s, loss=0.0977]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.78it/s]t/s, loss=0.0969]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 183.32it/s]t/s, loss=0.096]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.37it/s]t/s, loss=0.0952]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.95it/s]t/s, loss=0.0943]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.27it/s]t/s, loss=0.0935]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.66it/s]t/s, loss=0.0927]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 189.28it/s]t/s, loss=0.0919]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.42it/s]t/s, loss=0.0911]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.19it/s]t/s, loss=0.0903]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.59it/s]t/s, loss=0.0895]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.96it/s]t/s, loss=0.0888]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.08it/s]t/s, loss=0.088]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.99it/s]t/s, loss=0.0873]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.56it/s]t/s, loss=0.0865]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 179.85it/s]t/s, loss=0.0858]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 183.86it/s]t/s, loss=0.0851]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.50it/s]t/s, loss=0.0844]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.53it/s]t/s, loss=0.0837]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.78it/s]t/s, loss=0.083]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.75it/s]t/s, loss=0.0824]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.97it/s]t/s, loss=0.0817]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 179.53it/s]t/s, loss=0.081]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.81it/s]t/s, loss=0.0804]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.32it/s]t/s, loss=0.0797]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.12it/s]t/s, loss=0.0791]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.46it/s]t/s, loss=0.0785]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.12it/s]t/s, loss=0.0779]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.49it/s]t/s, loss=0.0772]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.12it/s]t/s, loss=0.0766]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.22it/s]t/s, loss=0.076]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.49it/s]t/s, loss=0.0755]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.01it/s]t/s, loss=0.0749]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 185.84it/s]t/s, loss=0.0743]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.90it/s]t/s, loss=0.0737]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.60it/s]t/s, loss=0.0732]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.26it/s]t/s, loss=0.0726]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.85it/s]t/s, loss=0.0721]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 183.25it/s]t/s, loss=0.0715]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.56it/s]t/s, loss=0.071]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.29it/s]t/s, loss=0.0705]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 79.90it/s]it/s, loss=0.0699]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.36it/s]t/s, loss=0.0694]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.49it/s]t/s, loss=0.0689]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.07it/s]t/s, loss=0.0684]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.19it/s]t/s, loss=0.0679]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.00it/s]t/s, loss=0.0674]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.37it/s]t/s, loss=0.0669]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.04it/s]t/s, loss=0.0665]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 195.31it/s]t/s, loss=0.066]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.95it/s]t/s, loss=0.0655]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.95it/s]t/s, loss=0.065]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.31it/s]t/s, loss=0.0646]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 184.67it/s]t/s, loss=0.0641]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.08it/s]t/s, loss=0.0637]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.86it/s]t/s, loss=0.0632]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.73it/s]t/s, loss=0.0628]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 188.93it/s]t/s, loss=0.0624]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.34it/s]t/s, loss=0.0619]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 183.21it/s]t/s, loss=0.0615]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 195.89it/s]t/s, loss=0.0611]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.55it/s]t/s, loss=0.0607]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.07it/s]t/s, loss=0.0603]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.71it/s]t/s, loss=0.0599]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.38it/s]t/s, loss=0.0595]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.60it/s]t/s, loss=0.0591]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.54it/s]t/s, loss=0.0587]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.96it/s]t/s, loss=0.0583]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.56it/s]t/s, loss=0.0579]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 179.17it/s]t/s, loss=0.0575]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.59it/s]t/s, loss=0.0571]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.94it/s]t/s, loss=0.0568]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.99it/s]t/s, loss=0.0564]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 191.69it/s]t/s, loss=0.056]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.56it/s]t/s, loss=0.0557]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 188.92it/s]t/s, loss=0.0553]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 187.02it/s]t/s, loss=0.055]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.42it/s]t/s, loss=0.0546]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.27it/s]t/s, loss=0.0543]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 188.36it/s]t/s, loss=0.0539]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.31it/s]t/s, loss=0.0536]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.48it/s]t/s, loss=0.0532]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.22it/s]t/s, loss=0.0529]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.73it/s]t/s, loss=0.0526]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 183.43it/s]t/s, loss=0.0522]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.29it/s]t/s, loss=0.0519]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.40it/s]t/s, loss=0.0516]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.25it/s]t/s, loss=0.0513]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.57it/s]t/s, loss=0.051]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.97it/s]t/s, loss=0.0507]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.96it/s]t/s, loss=0.0504]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 160.17it/s]t/s, loss=0.0501]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.71it/s]t/s, loss=0.0498]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.68it/s]t/s, loss=0.0495]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.06it/s]t/s, loss=0.0492]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.45it/s]t/s, loss=0.0489]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.93it/s]t/s, loss=0.0486]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 184.98it/s]t/s, loss=0.0483]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 180.67it/s]t/s, loss=0.048]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.12it/s]t/s, loss=0.0477]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.50it/s]t/s, loss=0.0474]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 182.81it/s]t/s, loss=0.0472]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.65it/s]t/s, loss=0.0469]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.36it/s]t/s, loss=0.0466]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 191.57it/s]t/s, loss=0.0463]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.55it/s]t/s, loss=0.0461]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 185.52it/s]t/s, loss=0.0458]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.93it/s]t/s, loss=0.0456]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 177.24it/s]t/s, loss=0.0453]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.19it/s]t/s, loss=0.045]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 184.81it/s]t/s, loss=0.0448]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 167.70it/s]t/s, loss=0.0445]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.86it/s]t/s, loss=0.0443]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.63it/s]t/s, loss=0.044]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 184.23it/s]t/s, loss=0.0438]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 176.28it/s]t/s, loss=0.0435]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 165.48it/s]t/s, loss=0.0433]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.26it/s]t/s, loss=0.0431]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.26it/s]t/s, loss=0.0428]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.86it/s]t/s, loss=0.0426]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.18it/s]t/s, loss=0.0424]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 128.96it/s]t/s, loss=0.0421]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 151.90it/s]t/s, loss=0.0419]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 171.69it/s]t/s, loss=0.0417]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 153.30it/s]t/s, loss=0.0415]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 149.81it/s]t/s, loss=0.0412]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 141.53it/s]t/s, loss=0.041]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 154.23it/s]t/s, loss=0.0408]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 136.42it/s]t/s, loss=0.0406]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 181.55it/s]t/s, loss=0.0404]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.84it/s]t/s, loss=0.0402]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.92it/s]t/s, loss=0.0399]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 151.45it/s]t/s, loss=0.0397]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.98it/s]t/s, loss=0.0395]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.47it/s]t/s, loss=0.0393]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.74it/s]t/s, loss=0.0391]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 178.19it/s]t/s, loss=0.0389]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.01it/s]t/s, loss=0.0387]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.13it/s]t/s, loss=0.0385]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 190.95it/s]t/s, loss=0.0383]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 164.68it/s]t/s, loss=0.0381]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 102.73it/s]t/s, loss=0.0379]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 154.20it/s]t/s, loss=0.0377]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.55it/s]t/s, loss=0.0375]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.47it/s]t/s, loss=0.0374]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 174.43it/s]t/s, loss=0.0372]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 161.97it/s]t/s, loss=0.037]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.18it/s]t/s, loss=0.0368]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.74it/s]t/s, loss=0.0366]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.00it/s]t/s, loss=0.0364]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.09it/s]t/s, loss=0.0362]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 179.72it/s]t/s, loss=0.0361]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.26it/s]t/s, loss=0.0359]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.47it/s]t/s, loss=0.0357]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 172.60it/s]t/s, loss=0.0355]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 175.80it/s]t/s, loss=0.0354]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 163.02it/s]t/s, loss=0.0352]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 170.09it/s]t/s, loss=0.035]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 173.94it/s]t/s, loss=0.0349]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 168.64it/s]t/s, loss=0.0347]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 150.15it/s]t/s, loss=0.0345]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 158.68it/s]t/s, loss=0.0344]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 155.01it/s]t/s, loss=0.0342]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 186.85it/s]t/s, loss=0.034]      \n",
      "100%|██████████| 1/1 [00:00<00:00, 162.31it/s]t/s, loss=0.0339]     \n",
      "100%|██████████| 1/1 [00:00<00:00, 169.36it/s]t/s, loss=0.0337]     \n",
      " 34%|███▍      | 345/1000 [00:11<00:21, 31.03it/s, train_accuracy=1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eric/ViT/notebooks/../vision_transformer/trainer.py:78\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     76\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 78\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, Y)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Eric/ViT/notebooks/../vision_transformer/vit.py:243\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    240\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# transformer\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the [CLS] token\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Eric/ViT/notebooks/../vision_transformer/vit.py:76\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders:\n\u001b[0;32m---> 76\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Eric/ViT/notebooks/../vision_transformer/vit.py:104\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mParams:\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    - x: (B, L, D)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m h \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m    105\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(x)) \u001b[38;5;241m+\u001b[39m h\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Eric/ViT/notebooks/../vision_transformer/vit.py:140\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# split into q k v vectors\u001b[39;00m\n\u001b[1;32m    138\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39munbind(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B x heads, L, dim_head)\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B x heads, L, dim_head)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    142\u001b[0m     x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b h) l d -> b l (d h)\u001b[39m\u001b[38;5;124m\"\u001b[39m, b\u001b[38;5;241m=\u001b[39mB, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\n\u001b[1;32m    143\u001b[0m )  \u001b[38;5;66;03m# (B x heads, L, dim_head) -> (B, L dim_head x heads)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# linear projection to dim\u001b[39;00m\n",
      "File \u001b[0;32m~/Eric/ViT/notebooks/../vision_transformer/vit.py:43\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(q, k, v)\u001b[0m\n\u001b[1;32m     40\u001b[0m d_k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     42\u001b[0m k_t \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, D, L)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43md_k\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m v  \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vit-mgwY0Bs6-py3.10/lib/python3.10/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-mgwY0Bs6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
